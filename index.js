import { openai } from '@ai-sdk/openai';
import { generateText, generateObject, stepCountIs, tool } from 'ai';
import { z } from 'zod';

import { prompts } from './prompts.js';

import 'dotenv/config';
import Exa from 'exa-js';

const mainModel = openai('gpt-4o-mini');
const exa = new Exa(process.env.EXASEARCH_API_KEY);


async function generateSearchQueries(subject, n = 4) {
  const {
    object: { queries },
  } = await generateObject({
    model: mainModel,
    system: prompts.manager.systemPrompt,
    prompt: `
      Generate ${n} concise search queries about the subject: ${subject}. The queries should enable a comprehensive press review by covering: breaking news, emerging trends, market dynamics, and influential opinions.
    `,
    schema: z.object({
      queries: z.array(z.string()).min(1).max(5),
    }),
  })

  return queries
}

const webSearch = tool({
  description: 'Search the web for up-to-date information',
  inputSchema: z.object({
    query: z.string(),
  }),
  execute: async ({ query }) => {
    // const { results } = await exa.searchAndContents(query, {
    //   livecrawl: 'always',
    //   numResults: 3,
    // });

    const results = [
      {
        "title": "Top 5 AI papers of August 2023 - LearnOpenCV",
        "url": "https://learnopencv.com/top-5-ai-papers-of-august-2023/",
        "content": "- [Skip to primary navigation](https://learnopencv.com/learnopencv.com#genesis-nav-primary)\n- [Skip to main content](https://learnopencv.com/learnopencv.com#genesis-content)\n- [Skip to primary sidebar](https://learnopencv.com/learnopencv.com#genesis-sidebar-primary)\n- [Skip to footer](https://learnopencv.com/learnopencv.com#genesis-footer-widgets)\n\n- [Home](https://learnopencv.com/)\n- [Getting Started](https://learnopencv.com/category/install/)\n - [Subscribe](https://learnopencv.com/)\n - [Installation](https://learnopencv.com/category/install/)\n - [Getting Started with OpenCV](https://learnopencv.com/getting-started-with-opencv/)\n - [PyTorch](https://learnopencv.com/getting-started-with-pytorch/)\n - [Tensorflow & Keras](https://learnopencv.com/getting-started-with-tensorflow-keras/)\n- [OpenCV University](https://opencv.org/university/?utm_source=lopcv&utm_medium=menu)\n - [CVDL Master Program](https://opencv.org/university/cvdl-master/?utm_source=lopcv&utm_medium=menu)\n - [Mastering OpenCV with Python](https://opencv.org/university/mastering-opencv-with-python/?utm_source=lopcv&utm_medium=menu)\n - [Fundamentals of CV & IP](https://opencv.org/university/fundamentals-of-computer-vision-and-image-processing/?utm_source=lopcv&utm_medium=menu)\n - [Deep Learning with PyTorch](https://opencv.org/university/deep-learning-with-pytorch/?utm_source=lopcv&utm_medium=menu)\n - [Deep Learning With TensorFlow & Keras](https://opencv.org/university/deep-learning-with-tensorflow-keras/?utm_source=lopcv&utm_medium=menu)\n - [Advanced Vision Applications with Deep Learning & Transformers NEW](https://opencv.org/university/advanced-vision-applications-with-deep-learning-and-transformers/?utm_source=locv&utm_medium=menu)\n - [Mastering Generative AI for Art](https://opencv.org/university/mastering-generative-ai-for-art/?utm_source=lopcv&utm_medium=menu)\n - [CV4Faces \\[Enrolled Users\\] (Old)](http://school-of-ai.teachable.com/)\n- [Free CoursesNEW](https://opencv.org/university/free-courses/?utm_source=lopcv&utm_medium=menu&utm_campaign=fcpage)\n - [VLM Bootcamp NEW](https://opencv.org/university/vision-language-model-bootcamp/?utm_source=lopcv&utm_medium=menu&utm_campaign=vlm)\n - [PyTorch Bootcamp FREE](https://opencv.org/university/free-pytorch-course/?utm_source=lopcv&utm_medium=menu&utm_campaign=ptbc)\n - [TensorFlow Keras Course FREE](https://opencv.org/university/free-tensorflow-keras-course/?utm_source=lopcv&utm_medium=menu&utm_campaign=tbc)\n - [OpenCV Course FREE](https://opencv.org/university/free-opencv-course/?utm_source=lopcv&utm_medium=menu&utm_campaign=obc)\n - [Python for Beginners FREE](https://opencv.org/university/python-for-beginners/?utm_source=lopcv&utm_medium=menu&utm_campaign=pbc)\n- Resources\n- [AI Consulting](https://learnopencv.com/computer-vision-machine-learning-artificial-intelligence-consulting/)\n- [About](https://learnopencv.com/about/)\n- [Youtube](https://bit.ly/YouTube_LearnOpenCV)\n\nX\n\nSearch ...\n\nResults\n\nSee all results\n\n[Home](https://learnopencv.com/)>[AI Research Papers](https://learnopencv.com/category/ai-research-papers/)> Top 5 AI papers of August 2023\n\n## Introduction\n\nIn the ever-evolving landscape of artificial intelligence, staying updated with the latest breakthroughs is essential. In this article, we will cover the **Top 5 AI papers of August 2023**. In July, we covered many exciting papers and projects at:\n\n- [Top 5 AI papers of July 2023](https://learnopencv.com/top-5-ai-papers-of-july-2023/)\n\nTop 5 AI papers of August 2023.\n\nAugust 2023 was full of exciting developments, and we’ve curated a list of the top 5 AI papers you need to know about. This month’s top papers address a range of challenges and offer practical solutions for both researchers and practitioners. Here’s a brief overview:\n\n### 3D Gaussian Splatting: An Alternative to Neural Radiance Fields\n\nThis paper presents an alternative to the popular neural radiance fields (NeRFs). This method for novel-view synthesis balances visual quality with real-time rendering capabilities. It introduces a technique using 3D Gaussians, offering a promising direction for those in graphics and visualization.\n\n### Pre-Trained Large Language Models for Industrial Control\n\nBefore large language models revolutionized AI, reinforcement learning was the most promising avenue for achieving general artificial intelligence. Unfortunately, apart from specific game like environments, the promise of RL has not materialized into agents which can operate in the real world. This paper presents a small scale alternative to reinforcement learning based on large language models. The problem setting is to control the heating system in a building. Instead of training agents to operate in this environment, the authors use a pretrained language model and ask it to reason about the environment. The results are impressive. Do read ahead to find out more.\n\n### The All-Seeing Project: Towards Panoptic Visual Recognition\n\nThe All-Seeing project combines vision and language in a unified framework. With a vast dataset and a model designed for panoptic visual recognition, it sets the stage for the next state-of-the-art foundation model.\n\n### Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP\n\nAlthough most readers are familiar with image segmentation, very few will be familiar with open vocabulary segmentation, i.e. image segmentation where the categories to be segmented are not known at training time. If you are wondering how this is even possible, please read on. This paper addresses the challenge of open-vocabulary segmentation and proposes a single-stage framework that simplifies and enhances the segmentation process, making it more efficient and applicable in real-time scenarios.\n\n### Composable Function-preserving Expansions for Transformer Architectures\n\nThis paper is for advanced engineers who want to optimize the architecture of their transformer neural network without incurring the extreme computational requirements of neural architecture search (NAS). Instead of deciding the network architecture before training, this research offers a method to progressively increase the parameters of transformer networks. As an engineer, this can potentially streamline the training process and provide you with a transformer network that is optimized for the task and dataset being used. Although the authors mostly refer to language models, the methods are general enough to be of great use in robotics where performance and computational cost both are critical.\n\nNow, let’s dive deep into each paper.\n\n[100K+ Learners\\\n3 Hours of Learning\\\n\\\n**Join Free OpenCV Bootcamp**](https://opencv.org/university/free-opencv-course/?utm_source=locv&utm_medium=midblog&utm_campaign=top-5-ai-papers-of-august-2023)\n\n[15K+ Learners\\\n3 Hours of Learning\\\n\\\n**Join Free TensorFlow Bootcamp**](https://opencv.org/university/free-tensorflow-keras-course/?utm_source=locv&utm_medium=midblog&utm_campaign=top-5-ai-papers-of-august-2023)\n\n[10K+ Learners\\\n8 Hours of Learning\\\n\\\n**Join Free PyTorch Bootcamp**](https://opencv.org/university/free-pytorch-course/?utm_source=locv&utm_medium=midblog&utm_campaign=top-5-ai-papers-of-august-2023)\n\n[View all AI Free Courses](https://opencv.org/university/free-courses/?utm_source=lopcv&utm_medium=blog)\n\n## Paper 1: 3D Gaussian splatting\n\nFigure 1. 3D Gaussian Splatting results.\n\n**Overview**: The paper titled “ **3D Gaussian Splatting for Real-Time Radiance Field Rendering**” is authored by Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. It introduces a novel method for real-time rendering of radiance fields, leveraging a 3D Gaussian scene representation.\n\n**Problem Addressed**: Radiance Field methods have recently transformed the novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality often requires neural networks that are expensive to train and render. Current methods either compromise on speed or quality. The challenge is to achieve real-time display rates for unbounded and complete scenes at 1080p resolution.\n\n**Methodology and Key Findings**: The authors propose three key elements:\n\n1. Representing the scene with 3D Gaussians, initialized from sparse points produced during camera calibration. This representation combines the advantages of **continuous volumetric radiance fields** and avoids unnecessary computation in empty spaces.\n2. Interleaved optimization/density control of the 3D Gaussians, optimizing anisotropic covariance to accurately represent the scene.\n3. A fast visibility-aware rendering algorithm that supports anisotropic splatting, accelerating both training and real-time rendering.\n\nThe results demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\n\n**Novel Ideas**: The paper introduces the concept of **anisotropic 3D Gaussians** as a high-quality representation of radiance fields. This representation allows for optimization with top-tier visual quality and competitive training times. Additionally, the paper presents a real-time rendering solution inspired by tile-based rasterization, which is visibility-aware and supports anisotropic splatting.\n\n**Implications**: The proposed method offers a significant improvement over NeRFs. It achieves equal or better quality than the best implicit radiance field approaches while providing faster training and real-time rendering. This approach can revolutionize how scenes captured with multiple photos are rendered in real time, making it a valuable tool for various applications in graphics and visualization.\n\n**Links:** Here are some relevant links for learning more about Gaussian splatting:\n\n- [Project page](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)\n- [Code](https://github.com/graphdeco-inria/gaussian-splatting)\n- [Paper](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/3d_gaussian_splatting_high.pdf)\n\n## Paper 2: Pre-Trained Large Language Models for Industrial Control\n\n**Overview:** The paper discusses the potential of foundation models, specifically large language models (LLMs) like GPT-4, in the domain of industrial control. The authors use **HVAC (Heating, Ventilation, and Air Conditioning)** building control as a case study to evaluate the performance of GPT-4 as a controller. **Problem Addressed:** Traditional reinforcement learning (RL) methods, commonly used for decision-making, face challenges such as sample inefficiency, high training costs, and the need for extensive training data. For industrial control tasks like HVAC control, there’s a need for high-performance controllers that can be developed with low technical debt, adapt quickly to new scenarios, and handle heterogeneous tasks efficiently.\n\nFigure 2. Pipeline showing GPT-4 being used for HVAC control.\n\n**Methodology and Key Findings:**\n\n- The authors propose a training-free method that leverages pre-trained LLMs for industrial control. This approach can handle various tasks with minimal samples since it doesn’t involve any training process.\n- The study focuses on controlling HVAC using GPT-4. The task is wrapped as a language game, where GPT-4 is provided with text prompts, including a short description of the task, selected demonstrations, and the current observation. GPT-4 then responds with actions.\n- Through a series of experiments, the authors sought to answer the questions:\n - How effectively can GPT-4 control HVAC?\n - How well can GPT-4 generalize to different HVAC control scenarios?\n - How do different parts of the text context influence performance?\n- The results indicate that GPT-4’s performance is comparable to traditional RL methods but requires fewer samples and has lower technical debt.\n\n**Novel Ideas:**\n\n- The paper introduces a unique approach to industrial control by directly using pre-trained LLMs without any additional training.\n- The authors design a mechanism to select demonstrations from both expert demonstrations and historical interactions. They also developed a prompt generator to transform various inputs into a prompt for the LLM.\n- The study provides insights into how different designs influence the performance of LLMs in industrial control tasks.\n\n**Implications:**\n\n- The research highlights the potential of foundation models, especially LLMs, in the realm of industrial control. These models can offer a viable alternative to traditional RL methods, especially in scenarios where low technical debt and quick adaptability are crucial.\n- The findings suggest that with proper prompting techniques, LLMs like GPT-4 can be effectively used for tasks like HVAC control, opening doors for their application in other industrial control scenarios.\n- The study also emphasizes the importance of in-context learning (ICL) for leveraging closed-source LLMs on specific tasks, indicating a possible future trend in the field.\n\n**Links:** The paper can be found at [Pre-trained Large Language Models for Industrial Control](https://arxiv.org/abs/2308.03028). The code is not available, unfortunately, but this paper is not difficult to implement.\n\n## Paper 3: The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World\n\n**Overview:** The paper introduces the **All-Seeing (AS)** project, which aims to recognize and understand everything in the open world through a large-scale data and model. The project focuses on achieving a comprehensive understanding of visual data, similar to human cognition.\n\nFigure 3. Comparison of All Seeing Model with other LLMs.\n\n**Problem Addressed:** The challenge lies in creating artificial general intelligence (AGI) systems that can match human intelligence across various domains. While Large Language Models (LLMs) have shown impressive capabilities in natural language processing tasks, they lack the ability to understand the visual world. Existing models and datasets primarily focus on understanding images as a whole, rather than recognizing individual instances within them.\n\n**Methodology and Key Findings:**\n\n- **Dataset (AS-1B):** The authors created a new dataset called AS-1B, which contains over 1 billion regions annotated with semantic tags, question-answering pairs, and detailed captions. This dataset covers 3.5 million common and rare concepts in the real world.\n- **All-Seeing Model (ASM):** A unified framework for panoptic visual recognition and understanding was developed. The model is trained with open-ended language prompts and locations, allowing it to generalize to various vision and language tasks.\n- **Performance:** The ASM demonstrated remarkable zero-shot performance in tasks like region-text retrieval, region recognition, captioning, and question-answering.\n\n**Novel Ideas:**\n\n- The introduction of a scalable data engine that incorporates human feedback and efficient models in the loop to create the AS-1B dataset.\n- The All-Seeing model (ASM) is a location-aware image-text foundation model that combines the capabilities of LLMs and visual models to recognize and understand objects or concepts in regions of interest.\n\n**Implications:** The All-Seeing project serves as a foundation for vision-language artificial general intelligence research. The creation of the **AS-1B dataset** and the **ASM** model can potentially revolutionize the field of visual recognition and understanding, bridging the gap between language and vision tasks. The project’s approach to combining human feedback with model annotations in a loop can also provide a blueprint for future large-scale dataset creation endeavors.\n\n**Links:** Here are some links for additional resources:\n\n- [Dataset Browser](https://huggingface.co/spaces/OpenGVLab/all-seeing)\n- [All Seeing Model Demo](https://openxlab.org.cn/apps/detail/wangweiyun/All-Seeing-Model-Demo)\n- [GitHub Code](https://github.com/OpenGVLab/all-seeing)\n- [The All Seeing Project Paper](https://arxiv.org/pdf/2308.01907.pdf)\n\n## Paper 4: Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP\n\n**Overview**: The paper presents a method for open-vocabulary segmentation using a single-stage framework called **FC-CLIP**. This framework is built on a shared **Frozen Convolutional CLIP** backbone.\n\nFigure 4. Open vocabulary segmentation architecture.\n\n**Problem Addressed**: This requires some background. The usual setting of image segmentation is that the categories being segmented are known in advance and each category has a specific index. At test time, the network predicts the mask and category index of each pixel. However, in completely open and diverse environments, the categories may not be known at training time but only at test time. The challenge is to predict the mask and label of such objects in images. The way this is usually done is via a two-stage approach. One stage predicts the mask of the object using a general-purpose class agnostic segmentation network (such as the SAM model). The second stage crops the image using the predicted mask and inputs the image to a CLIP model, which predicts the class as a text token. Thus, the class label is not an integer index but a text-based token. Thus, any class can be handled as long as it exists in the vocabulary of the CLIP model. Such two-stage frameworks can be inefficient. The paper aims to simplify this process by integrating everything into a single-stage framework.\n\n**Methodology and Key Findings**:\n\n- The FC-CLIP framework consists of three modules built upon a shared frozen convolutional CLIP backbone: a class-agnostic mask generator, an in-vocabulary classifier, and an out-of-vocabulary classifier.\n- The frozen CLIP backbone ensures that the pretrained image-text feature alignment remains intact, allowing for out-of-vocabulary classification.\n- The convolutional CLIP, based on a Convolutional Neural Network (CNN), shows better generalization ability compared to ViT-based CLIP when the input size scales up.\n- FC-CLIP achieves state-of-the-art performance on multiple benchmarks, surpassing prior methods. For instance, when trained on the COCO panoptic dataset only, FC-CLIP achieves significant improvements in performance metrics on datasets like ADE20K, Mapillary Vistas, and Cityscapes.\n\n**Novel Ideas**:\n\n- The introduction of a single-stage framework, FC-CLIP, that uses a shared Frozen Convolutional CLIP backbone.\n- The observation that the frozen CLIP backbone can serve both as an open-vocabulary classifier and a strong mask generator.\n- The finding that convolutional CLIP generalizes better to larger input resolutions than ViT-based CLIP.\n\n**Implications**:\n\n- The FC-CLIP framework simplifies the process of open-vocabulary segmentation, making it more efficient and effective.\n- The method sets a new benchmark for open-vocabulary segmentation, outperforming existing two-stage methods.\n- The study provides insights into the potential of using a single frozen convolutional CLIP for various segmentation tasks, paving the way for future research in this area.\n\n**Links:** Here are some links to learn more:\n\n- [Open Vocabulary Segmentation Code](https://github.com/bytedance/fc-clip)\n- [Open Vocabulary Segmentation Paper](https://arxiv.org/pdf/2308.02487.pdf)\n\n## Paper 5: Composable Function-preserving Transformations for Transformer Architectures\n\n**Overview:** This paper introduces a set of transformations to incrementally scale transformer-based neural networks without losing their functionality.\n\nFigure 5. Standard transformer neural network.\n\n**Problem Addressed:** The challenge in the field of neural networks is the computational and time cost associated with training state-of-the-art models. Typically, scaling up a neural network necessitates starting from scratch, which means the loss of knowledge acquired by previously trained models. The paper seeks to address this inefficiency by proposing a method to expand the architecture of transformer-based models without compromising their function. This can equivalently be used to optimize network architecture without having to do a neural architecture search (as long as you limit yourself to the family of transformer networks).\n\n**Methodology and Key Findings:**\n\nThe authors present six distinct transformations targeting different hyper-parameters of the transformer architecture. These transformations allow for the expansion of the model in terms of:\n\n1. Size of MLP internal representation\n2. Number of attention heads\n3. Size of the attention heads output representation\n4. Size of the attention input representation\n5. Size of the transformer layers’ input/output representations\n6. Number of layers\n\nEach transformation is accompanied by a proof ensuring that the function of the model remains unchanged, given certain initialization constraints for the added parameters. If you have followed along with our previous posts about implementing [transformers](https://learnopencv.com/the-future-of-image-recognition-is-here-pytorch-vision-transformer/) and the [attention mechanism](https://learnopencv.com/attention-mechanism-in-transformer-neural-networks/) from scratch in PyTorch, this whole paper will be very easy to understand and, in fact, almost trivial!\n\n**Novel Ideas:** The paper’s novelty lies in its comprehensive and composable set of function-preserving transformations for transformer like neural networks. While previous works have touched upon similar concepts, this framework stands out due to its thoroughness and the breadth of transformations it covers. The proposed transformations are designed to be simple yet minimally constraining, offering flexibility in scaling transformer architectures.\n\n**Implications:**\n\nThe proposed transformations have significant implications for the training and optimization of neural networks. They offer a pathway to efficiently scale models without starting from scratch, potentially leading to cost and time savings. In future applications, these transformations can be utilized to train larger models by beginning with a smaller model and progressively expanding its architecture. Additionally, they can be used to create a family of models of varying sizes, all stemming from a common training checkpoint. The paper also suggests the potential integration of neural architecture search (NAS) techniques to determine the optimal transformation scheduling and architectural progression tailored to specific tasks and computational budgets.\n\n**Links:** The paper can be found at [Composable Function Preserving Transformations](https://arxiv.org/pdf/2308.06103.pdf), and the code is publicly available at [Jupyter Notebook for function preserving transformations](https://github.com/google-research/google-research/blob/master/muNet/TransformerExpansions.ipynb).\n\n## Summary\n\nThe purpose of this series of blog posts is not to explain all the details of each paper comprehensively but to keep you updated about the major findings and provide a trigger to dive deeper into papers relevant to your work. In this spirit, let us summarize the papers in one sentence each!\n\n1. **3D Gaussian Splatting:** This paper introduces an innovative approach to novel-view synthesis, achieving state-of-the-art visual quality and real-time rendering using 3D Gaussians, which outperforms traditional radiance field methods.\n2. **Pre-Trained Large Language Models for Industrial Control:** The research explores the potential of GPT-4, a foundation model, in controlling HVAC systems, demonstrating its comparable performance to RL methods and highlighting its applicability in industrial control tasks.\n3. **The All-Seeing Project:** This ambitious project presents a large-scale data and model aimed at recognizing and understanding everything in the open world, serving as a potential cornerstone for vision-language artificial general intelligence research.\n4. **Convolutions Die Hard:** The paper proposes a single-stage framework, FC-CLIP, for open-vocabulary segmentation, simplifying the traditional two-stage pipeline and achieving state-of-the-art performance across various datasets with increased efficiency.\n5. **Composable Function-preserving Expansions for Transformer Architectures:** This work offers a novel method to incrementally scale transformer-based neural networks without starting from scratch, presenting six composable transformations that preserve the model’s functionality.\n\nWe hope you found these insights valuable. Stay tuned and come back next month for September’s top 5 papers, where we’ll continue to bring you the latest and most impactful research in the field.\n\nLoad Comments\n\n## Get Started with OpenCV\n\n- FREE OpenCV Crash Course\n- Getting Started Guides\n- Installation Packages\n- C++ And Python Examples\n- Newsletter Insights\n\nName\n\nEmail\n\nGet Started\n\n- We hate SPAM and promise to keep your email address safe.\n\n## Subscribe to receive the download link, receive updates, and be notified of bug fixes\n\n## Which email should I send you the download link?\n\nFirst Name\n\nEmail\n\nPost Title\n\nCode URL\n\nDownload the code\n\n- We hate SPAM and promise to keep your email address safe.\n\nSubscribe To Receive\n\n- FREE OpenCV Crash Course\n- Getting Started Guides\n- Installation Packages\n- C++ And Python Examples\n- Newsletter Insights\n\nWe hate SPAM and promise to keep your email address safe.​\n\nName\n\nEmail\n\nGet Started\n\n- We hate SPAM and promise to keep your email address safe.\n\nSubscribe Now\n\nName\n\nEmail\n\nSubmit\n\nAbout LearnOpenCV\n\nEmpowering innovation through education, LearnOpenCV provides in-depth tutorials, code, and guides in AI, Computer Vision, and Deep Learning. Led by Dr. Satya Mallick, we're dedicated to nurturing a community keen on technology breakthroughs.\n\n[Read More](https://learnopencv.com/about/)\n\nFREE Courses\n\nCategories\n\nGetting Started\n\nOpenCV University\n\nCopyright © 2025 – BIG VISION LLC [Privacy Policy](https://learnopencv.com/privacy-policy/) [Terms and Conditions](https://learnopencv.com/terms-and-conditions/)",
        "publishedDate": "2023-09-12T13:00:00.000Z"
      },
      {
        "title": "August 2023: A Month of Remarkable Advancements in AI",
        "url": "https://www.moondeavors.com/post/august-2023-a-month-of-remarkable-advancements-in-ai",
        "content": "top of page\n\n# The AI Blog\n\ninformation about the rapid developments of AI to teens, from a teen :).\n\nMost technical information comes from outside sources but summarized into my own words for easier understanding for the readers.\n\nPost: Text\n\n## Resources Used\n\n[Artificial intelligence \\| MIT News \\| Massachusetts Institute of Technology](https://news.mit.edu/topic/artificial-intelligence2)\n\n[AI \\| TechCrunch](https://techcrunch.com/category/artificial-intelligence/)\n\n​[Artificial Intelligence - Latest News and Analysis - WSJ.com](https://www.wsj.com/tech/ai)\n\nPost: Text\n\nSearch\n\nIt is almost the end of August, which means it is the monthly wrap-up!\n\nArtificial Intelligence (AI) continues to evolve at a breathtaking pace, shaping the way we live, work, and interact with technology. In August 2023, we're greeted with exciting developments that showcase the ongoing transformation of our digital landscape. From healthcare breakthroughs to novel AI applications in various sectors, let's explore some of the most noteworthy AI advancements in the month of August 2023!\n\n**AI-Powered Healthcare Breakthroughs:**\n\nIn August 2023, AI made significant strides in the healthcare sector. Researchers unveiled a groundbreaking AI system that can predict various diseases by analyzing the subtle changes in a patient's voice. This innovative application of AI could revolutionize early diagnosis, potentially saving lives and reducing healthcare costs.\n\n**AI in Climate Change Mitigation:**\n\nAs concerns about climate change escalate, AI continues to play a crucial role in addressing environmental challenges. In August, AI-powered predictive models gained recognition for their ability to forecast weather patterns and natural disasters more accurately. These models provide invaluable information to help communities prepare for and respond to extreme weather events.\n\n**AI-Driven Education Solutions:**\n\nEducation received a technological boost in August 2023, with the introduction of AI-driven tutoring platforms that adapt to individual learning styles. These platforms offer personalized, real-time feedback to students, enhancing their educational experience. Furthermore, educators benefited from AI-powered administrative tools, streamlining tasks and allowing for more effective teaching.\n\n**AI-Enhanced Entertainment:**\n\nAI continued to impact the entertainment industry, with artists, musicians, and content creators utilizing AI-generated content. Music composers integrated AI-generated melodies and lyrics into their compositions, pushing the boundaries of creativity. Additionally, AI systems created visual art, expanding the horizons of what is possible in the realm of human-AI collaboration.\n\n**AI in Financial Services:**\n\nAI played a pivotal role in the financial sector in August 2023. Robo-advisors, equipped with advanced AI algorithms, continued to gain popularity among investors seeking personalized financial advice. These platforms adapted to market conditions in real-time, maximizing returns on investments. Additionally, AI-powered fraud detection tools provided enhanced security for financial institutions and their clients.\n\n**Ethical AI Advancements:**\n\nThe discussion surrounding ethical AI usage gained momentum in August 2023. Organizations and policymakers collaborated to establish robust guidelines for the responsible and ethical development and deployment of AI systems. These efforts included the creation of AI ethics councils and regulatory frameworks aimed at ensuring transparency, fairness, and accountability in AI applications.\n\nAugust 2023 was a remarkable month for artificial intelligence, witnessing advancements across various sectors. From healthcare and climate change mitigation to education and entertainment, AI demonstrated its transformative potential.\n\nIn recent years, AI-driven conversation agents have shifted from being futuristic concepts to everyday assistants. ChatGPT, a prime...\n\nPost: Blog2 Post\n\nbottom of page",
        "publishedDate": "2023-08-29T07:00:00.000Z"
      },
      {
        "title": "AI Eng Recap: August 2023 - Latent.Space",
        "url": "https://www.latent.space/p/aug-2023",
        "content": "SubscribeSign in\n\n# AI Eng Recap: August 2023\n\n### Our highest signal selection of the most relevant items for AI Engineers\n\n[Latent.Space](https://substack.com/@swyx)\n\nSep 01, 2023\n\n∙ Paid\n\n49\n\n6\n\n[Share](javascript:void(0))\n\n_We’re putting the “news” back in “newsletter”. Presenting our first monthly recap of everything we’ve heard that is relevant to the AI Engineer. This is free for current subscribers and then [moved to…](https://www.latent.space/subscribe)_\n\n## Keep reading with a 7-day free trial\n\nSubscribe to Latent.Space to keep reading this post and get 7 days of free access to the full post archives.\n\n[Start trial](https://www.latent.space/subscribe?simple=true&next=https%3A%2F%2Fwww.latent.space%2Fp%2Faug-2023&utm_source=paywall-free-trial&utm_medium=web&utm_content=136642587&coupon=5fe099d9)\n\n[Already a paid subscriber? **Sign in**](https://substack.com/sign-in?redirect=%2Fp%2Faug-2023&for_pub=swyx&change_user=false)\n\nPreviousNext",
        "publishedDate": "2023-09-01T19:18:34.000Z"
      }
    ];

    return results.map(result => ({
      title: result.title,
      url: result.url,
      content: result.text,
      publishedDate: result.publishedDate,
    }));
  },
});

async function researchSubject(subject) {
  const result = await generateText({
    model: mainModel,
    prompt: `Search the web for the latest developments in the ${subject} and return a raport from the gathered data`,
    system: prompts.researcherSystemPrompt,
    tools: {
      webSearch,
    },
    stopWhen: stepCountIs(3),
  });
}

function validateSecrets() {
  const secrets = ['OPENAI_API_KEY', 'EXASEARCH_API_KEY'];

  for(const secret of secrets) {
    if (!process.env[secret]) {
      console.error(`Error: Missing ${secret}. Please set it in your environment or .env file.`);

      process.exit(1);
    }
  }
}

async function main() {
validateSecrets();
  
  // Read subject from CLI; default to DEFFAULT_SUBJECT if not provided
  const args = process.argv.slice(2);
  let subject = args.join(' ').trim();
  if (!subject) {
    console.log(`No subject provided via CLI. Defaulting to '${process.env.DEFAULT_SUBJECT}'.`);
    subject = process.env.DEFAULT_SUBJECT;``
  }
  
const queries = await generateSearchQueries(subject);

  console.log(JSON.stringify(queries));

  //   const research = await researchSubject(subject);

//   console.log(research.text);
}

main();

